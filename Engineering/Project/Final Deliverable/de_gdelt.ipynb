{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bed92710",
   "metadata": {},
   "source": [
    "**Get Headlines from GDELT API, Translate, Clean, Get Sentiment / Polarity, and Store in Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4139050",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "029d6b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "import time, requests, re\n",
    "\n",
    "from sqlalchemy  import create_engine\n",
    "from textblob    import TextBlob\n",
    "from googletrans import Translator, constants\n",
    "from bs4         import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "437b01a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year  = 2021 #YYYY\n",
    "end_year    = 2021 #YYYY\n",
    "start_month = 9   #MM\n",
    "end_month   = 9   #MM\n",
    "start_day   = 1    #DD\n",
    "end_day     = 1  #DD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15baafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine       = create_engine(\"sqlite:///hl_example.db\")\n",
    "translator   = Translator()\n",
    "alphanumeric = lambda x: re.sub(r'[^A-Za-z ]+', '', x)\n",
    "lowcase      = lambda x: x.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6538f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_links(sdt, edt): # YYYYMMDDHHMMSS\n",
    "    url = 'https://api.gdeltproject.org/api/v2/doc/doc?query=\"COP26\"&mode=artlist&maxrecords=250&startdatetime='+sdt+'&enddatetime='+edt\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    page = response.text\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    return page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "167fc65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/soupsieve/css_parser.py:813: FutureWarning: The pseudo class ':contains' is deprecated, ':-soup-contains' should be used moving forward.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for year in range(start_year, end_year + 1, 1):\n",
    "    \n",
    "    for month in range(start_month, end_month + 1, 1):\n",
    "    \n",
    "        for day in range(start_day, end_day + 1, 1):\n",
    "\n",
    "            for hour in range(0, 24, 1):\n",
    "                \n",
    "                links=pd.DataFrame(columns=['url',\n",
    "                                            'arttitle',\n",
    "                                            'source',\n",
    "                                            'date_time', \n",
    "                                            'language', \n",
    "                                            'country']\n",
    "                                  )\n",
    "\n",
    "                start  = str.zfill(str(hour),2)\n",
    "                days   = str.zfill(str(day),2)\n",
    "                months = str.zfill(str(month),2)\n",
    "                years  = str.zfill (str(year),4)\n",
    "                \n",
    "                sdt    = years + months + days + start + '0000'\n",
    "                edt    = years + months + days + start + '5959'\n",
    "\n",
    "                page = get_links(sdt, edt)\n",
    "\n",
    "                soup = BeautifulSoup(page, \"html5lib\")\n",
    "\n",
    "                for index, link in enumerate(soup.find_all('a')):\n",
    "                    urls = link.get(\"href\")\n",
    "                    links.loc[index,'url'] = urls\n",
    "\n",
    "                for index, span in enumerate(soup.find_all(\"span\", class_=\"arttitle\")):\n",
    "                    arttitles = span.text\n",
    "                    links.loc[index,'arttitle'] = arttitles\n",
    "\n",
    "                for index, span in enumerate(soup.find_all(\"span\", class_=\"sourceinfo\")):\n",
    "                    sourceinfos = span.text.split()\n",
    "                    links.loc[index,'source'] = sourceinfos[0]\n",
    "                    links.loc[index,'language'] = sourceinfos[5]\n",
    "                    try:\n",
    "                        try:\n",
    "                            links.loc[index,'country'] = sourceinfos[6] + \" \" + sourceinfos[7]\n",
    "                        except:\n",
    "                            links.loc[index,'country'] = sourceinfos[6]\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                script = soup.select_one('script:contains(\"sourceinfo_date\")')\n",
    "                date = re.findall(r'[0-9][0-9]/[0-9][0-9]/[0-9][0-9][0-9][0-9] [0-9][0-9]:[0-9][0-9]',script.text)\n",
    "                links['date_time'] = date\n",
    "                pd.to_datetime(links['date_time'])\n",
    "                  \n",
    "                links['arttitle_en'] = links['arttitle']\n",
    "                \n",
    "                for index in range(len(links)):\n",
    "                    if links['language'].iloc[index] != 'English':\n",
    "                        try:\n",
    "                            text = links['arttitle'].iloc[index]\n",
    "                            translation = translator.translate(text)\n",
    "                            links['arttitle_en'].iloc[index] = translation.text\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        pass\n",
    "                    \n",
    "                links['clean'] = links['arttitle_en']\n",
    "                links['clean'] = links.clean.map(alphanumeric)\n",
    "                links['clean'] = links.clean.map(lowcase)\n",
    "                \n",
    "                links['sentiment'] = np.nan\n",
    "                links['polarity']  = np.nan\n",
    "                \n",
    "                for index, hl in enumerate(links['clean']):\n",
    "\n",
    "                    links.loc[index,'sentiment'] = TextBlob(hl).sentiment.subjectivity\n",
    "                    links.loc[index,'polarity'] = TextBlob(hl).sentiment.polarity\n",
    "\n",
    "                links.to_sql('headlines', con = engine, schema=None, if_exists='append', index=True, index_label=None, chunksize=None, dtype=None, method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95274d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4141b43f",
   "metadata": {},
   "source": [
    "**Vectorize Headline Excluding Stop Words and Determine Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83d1bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b195b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition           import NMF\n",
    "from sqlalchemy                      import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37078d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(\"sqlite:///hl_example.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e41e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "arttitle_cl = pd.read_sql('SELECT clean, country FROM headlines;', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b39def9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-808e98acd981>:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  arttitle_cl['clean'] = arttitle_cl['clean'].str.replace('(?:biden|obama|greta|boris|cop|climate|modi|william|charles|merkel|conference|news|john|xi|terry|envoy|chief|jinping|mccrann|minister|sturgeon|nicola|pm|prince|sadyr|japarov|catriona|stewart|world|leaders|morrison|kerry|summit|change|glasgow|president|elizabeth|queen|pope|francis|johnson|thunberg|iain|macwhirter)', '')\n"
     ]
    }
   ],
   "source": [
    "arttitle_cl['clean'] = arttitle_cl['clean'].str.replace('(?:biden|obama|greta|boris|cop|climate|modi|william|charles|merkel|conference|news|john|xi|terry|envoy|chief|jinping|mccrann|minister|sturgeon|nicola|pm|prince|sadyr|japarov|catriona|stewart|world|leaders|morrison|kerry|summit|change|glasgow|president|elizabeth|queen|pope|francis|johnson|thunberg|iain|macwhirter)', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13fba85c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Incorporate stop words when creating the count vectorizer\n",
    "\n",
    "vec = TfidfVectorizer(stop_words='english', min_df = 10)\n",
    "X = vec.fit_transform(arttitle_cl.clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d4f7c8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_tokens = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa97e2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_words = news_tokens.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "229e9f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    }
   ],
   "source": [
    "nmf_model = NMF(20)\n",
    "news_topics = nmf_model.fit_transform(news_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4660205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    \n",
    "    topics=pd.DataFrame(columns=['topic'])\n",
    "    \n",
    "    for index, topic in enumerate(model.components_):           \n",
    "        topics.loc[index,'topic'] = '{}: '.format(index) + ' '.join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0326a1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_topics = pd.DataFrame(news_topics.round(5),\n",
    "             columns = display_topics(nmf_model, vec.get_feature_names(), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "213689b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = map_topics.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd0ade37",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_str = []\n",
    "\n",
    "for c in columns:\n",
    "    columns_str.append(c[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f114c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_topics.columns = columns_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc0df4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_topic = map_topics.idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0a308b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_hl_topic = pd.concat([arttitle_cl, top_topic], axis=1)\n",
    "news_hl_topic.rename(columns={0:'Topic'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37e2abe4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news_hl_topic.to_sql('topics', con = engine, schema=None, if_exists='replace', index=True, index_label=None, chunksize=None, dtype=None, method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843303f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
